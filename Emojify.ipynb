{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emojify.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAlexSanz/Emojify/blob/master/Emojify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luQ4vH5XT5qM",
        "colab_type": "code",
        "outputId": "46ab04e5-4615-4648-c8fc-4000c1c8f0ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "%cd \"/content\"\n",
        "!rm -rf Emojify\n",
        "\n",
        "!git clone https://github.com/DrAlexSanz/Emojify.git\n",
        "  \n",
        "%cd \"/content/Emojify\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Emojify'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 23 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "Unpacking objects: 100% (23/23), done.\n",
            "/content/Emojify\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF7QaqIBUPjI",
        "colab_type": "code",
        "outputId": "85bedaa9-a772-4762-a60c-c05452344e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip install emoji\n",
        "import numpy as np\n",
        "import emoji\n",
        "from emo_utils import *\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 10kB 14.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 30kB 8.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40kB 5.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 5.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=3ca78b5d9980e3c6739d1e62df3b766d206bfe50f38305ac039b0ed82d1c61be\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR4pGOCqNVuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = read_csv(\"Data/train_emoji.csv\")\n",
        "X_test, y_test = read_csv(\"Data/tesss.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yflu__5XNvkT",
        "colab_type": "text"
      },
      "source": [
        "Now I have all the data read, train and test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4ur_mr7U-9j",
        "colab_type": "code",
        "outputId": "6ceee614-a3ef-4c59-ccbf-e9bd453cb567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "index = 13\n",
        "\n",
        "print(X_train[index], label_to_emoji(y_train[index]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my algorithm performs poorly üòû\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5HU1ZU1VRrI",
        "colab_type": "text"
      },
      "source": [
        "This is a bit pessimistic. At least I'm not, but if you are, change the index and find a nicer one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfZSQA-PWlma",
        "colab_type": "text"
      },
      "source": [
        "Since the labels are numerical, 0, 1, 2, etc. I have to convert them to one-hot encoding. I just happen to have a function for this. How convenient and how elegant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vZuCqMyW3RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_oh_train = convert_to_one_hot(y_train, C = 5)\n",
        "Y_oh_test = convert_to_one_hot(y_test, C = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK2dl8IfXHH1",
        "colab_type": "code",
        "outputId": "ba414d5c-824d-4227-f20a-67418158640a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To test\n",
        "index = 13\n",
        "print(str(y_train[index]) + \" converted to one-hot is \" + str(Y_oh_train[index]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 converted to one-hot is [0. 0. 0. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnk-WVoEYIt4",
        "colab_type": "text"
      },
      "source": [
        "Now one thing I should do is to read the GloVe vectors. I have them somewhere in google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9xS-cD1YO7P",
        "colab_type": "code",
        "outputId": "3158be16-3057-46a7-885e-8fc4433e2bd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpYEqLrzYiBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(\"/content/drive/My Drive/Glove/glove.6B.50d.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5sU8b80Y593",
        "colab_type": "text"
      },
      "source": [
        "As a reminder:\n",
        "\n",
        "\n",
        "*   word to index is a dictionary mapping, from word to the indices of the vocabulary.\n",
        "*   index to word is the inverse mapping.\n",
        "*   word to vec is the mapping of word towards vector.\n",
        "\n",
        "If I want an index to vec mapping I need ot do it in two steps, but just that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6m2Zhn4ZdUr",
        "colab_type": "code",
        "outputId": "ee8970de-3e16-4232-b602-f1633c748811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Check this. Words are uncased!!\n",
        "\n",
        "word = \"cucumber\"\n",
        "index =325651\n",
        "\n",
        "print(\"The index of \" + word + \" is: \" + str(word_to_index[word]))\n",
        "print(\"The word for index \" + str(index) + \" is \" + str(index_to_word[index]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The index of cucumber is: 113317\n",
            "The word for index 325651 is serpent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LdtKjhIlHpq",
        "colab_type": "text"
      },
      "source": [
        "Implement sentence to average. Take a sentence, split it and get the average of the 50D encoding. This will be useful later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmKjffAglZRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_to_avg(sentence, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    The function takes a sentence, makes it lowercase and split it and then convert the words to the vector repesentation\n",
        "    Then the vectors are averaged\n",
        "    \n",
        "    Inputs: Sentence (string) and word_to_vec mapping\n",
        "    \n",
        "    Output: The average of the vector encoding    \n",
        "    \"\"\"\n",
        "    \n",
        "    # Split and lowercase\n",
        "    \n",
        "    words = [i.lower() for i in sentence.split()]\n",
        "    \n",
        "    #Initialize average\n",
        "    avg = np.zeros((50,))\n",
        "    \n",
        "    for w in words:\n",
        "        avg = avg + word_to_vec_map[w]\n",
        "    \n",
        "    avg = avg / len(words)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT9y77R2mse6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cipote = sentence_to_avg(\"I am freezing my balls off\", word_to_vec_map)\n",
        "print(cipote)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBSbVnc0FHj5",
        "colab_type": "text"
      },
      "source": [
        "So with this function, I'll take the average output and do the forward/ backward cycle. I use the cross-entropy as the loss and the y predicted is assumed to be the one-hot encoding of the labels.\n",
        "\n",
        "$$ z^{(i)} = W . avg^{(i)} + b$$$$ a^{(i)} = softmax(z^{(i)})$$$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXYsMQMoFc8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X, Y, word_to_vec_map, learning_rate, num_iterations):\n",
        "    \"\"\"\n",
        "    This function trains the model itself.\n",
        "    \n",
        "    inputs. X, Y are the data + labels, (m, 1) in shape and Y is NOT the One-Hot encoding. Other inputs are self explanatory.\n",
        "    \n",
        "    Output: pred is the vector of predictions, (m, 1)\n",
        "            W is the weight matrix of the softmax layer (last one). Shape (n_y, n_h)\n",
        "            b is the bias of the softmax layer. Shape (n_y,)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    \n",
        "    # Get shapes\n",
        "    \n",
        "    m = Y.shape[0]\n",
        "    n_y = 5 # Classes!!\n",
        "    n_h = 50 # This is from the glove vectors.\n",
        "    \n",
        "    # Initialize W, b using Xavier initialization. Manually because I can.\n",
        "    \n",
        "    W = np.random.randn(n_y, n_h)/np.sqrt(n_h)\n",
        "    b = np.random.randn(n_y,)\n",
        "    \n",
        "    # Convert to one hot.\n",
        "    \n",
        "    Y_oh = convert_to_one_hot(Y, C = n_y)\n",
        "    \n",
        "    # The optimization loop. Loop over every example and then do it for n iterations.\n",
        "    \n",
        "    for iter in range(num_iterations):\n",
        "        for i in range(m):\n",
        "            \n",
        "            # Get average for input\n",
        "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
        "            \n",
        "            # Forward prop. Check the equations above.\n",
        "            z = np.dot(W, avg) + b\n",
        "            a = softmax(z)\n",
        "            \n",
        "            # Compute cost\n",
        "            cost = -np.sum(np.multiply(Y_oh[i], np.log(a)))\n",
        "            \n",
        "            # Gradients! Check it by hand because this is not obvious.\n",
        "            dz = a - Y_oh[i]\n",
        "            dW = np.dot(dz.reshape(n_y, 1), avg.reshape(1, n_h))\n",
        "            db = dz\n",
        "            \n",
        "            # Update\n",
        "            W = W - learning_rate * dW\n",
        "            b = b - learning_rate * db\n",
        "            \n",
        "            # Print some of them\n",
        "            \n",
        "        if iter % 100 == 0:\n",
        "            print(\"At iteration \" + str(iter) + \" the cost is \" + str(cost))\n",
        "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
        "\n",
        "    return pred, W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxZmH20yKJ0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = 0.01\n",
        "num_iter = 400\n",
        "\n",
        "pred, W, b = model(X_train, y_train, word_to_vec_map, learn, num_iter)\n",
        "# print(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}